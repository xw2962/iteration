---
title: "writing_functions"
author: "Xiaoyu Wu"
date: "2023-10-25"
output: github_document
---

```{r setup}
library(tidyverse)
library(rvest)

set.seed(12345)
# set seed for reproducibility 
```

## Do something simple 
```{r}
x_vec = rnorm(30, mean = 5, sd = 3)
# 30 vectors taking the mean of 5 and sd of 3 
(x_vec - mean(x_vec)) / sd(x_vec)
# z-scores: subtracted the mean and divided by the sd
```
I wanta function to compute z-scores 
```{r}
z_scores = function(x) {
# argument is x and inside the brackets 
# input is x everywhere 
  z = (x - mean(x)) / sd(x)
# body and operation 
  return(z)
# return objects   
}

z_scores(x=x_vec)
```
Try my functions on some other thing 
```{r}
z_scores = function(x) {
  
  if (!is.numeric(x)) {
    stop("Argument x should be numeric")
  } else if (length(x) == 1) {
    stop("Z scores cannot be computed for length 1 vectors")
  }
# update teh function 
  z = mean(x) / sd(x)
  
  z
}
# add some checks 
```

## Multiple outputs 
Write a function that returns the mean and sd from a sample of numbers. 
store each of the values in a named list
```{r}
mean_and_sd = function(x) {
  
  if (!is.numeric(x)) {
    stop("Argument x should be numeric")
  } else if (length(x) == 1) {
    stop("Cannot be computed for length 1 vectors")
  }
  
  mean_x = mean(x)
  sd_x = sd(x)

  list(mean = mean_x, 
       sd = sd_x)
}
mean_and_sd(x_vec)
```
store values in a data frame
```{r}
mean_and_sd = function(x) {
  
  if (!is.numeric(x)) {
    stop("Argument x should be numeric")
  } else if (length(x) == 1) {
    stop("Cannot be computed for length 1 vectors")
  }
  
  mean_x = mean(x)
  sd_x = sd(x)
# operations
  tibble(
    mean = mean_x, 
    sd = sd_x
  )
# return tibble data-frame as results 
}
mean_and_sd(x_vec)
```

## Multiple inputs
```{r}
sim_data = tibble(
  x = rnorm(30, mean = 2, sd = 3)
)

sim_data %>% 
  summarize(
    mu_hat = mean(x),
    sigma_hat = sd(x)
  )
```
I'd like to do this in a function 
```{r}
sim_mean_sd = function(sample_size, mu, sigma) {
# true population mean mu and true population sd sigma.
# (sample_size, mu=3, sigma=4) default values 
  sim_data = tibble(
    x = rnorm(n=sample_size, mean = mu, sd = sigma),
  )
# inputs  
  sim_data %>% 
    summarize(
      mu_hat = mean(x),
      sigma_hat = sd(x)
    )
# return objects mu_hat and sigma_hat 
}
sim_mean_sd(sample_size=100,mu=6,sigma=3)
```
different way of writing this 
```{r}
sim_mean_sd_tibble = function(sample_size, mu, sigma) {
# true population mean mu and true population sd sigma.
# (sample_size, mu=3, sigma=4) default values 
  x_vec=rnorm(n=sample_size, mean = mu, sd = sigma) 
# inputs  
  tibble(
    mean = mean(x_vec),
    sd = sd(x_vec)
    )
# return object tibble  
}
sim_mean_sd_tibble(sample_size=100,mu=6,sigma=3)
```

## Revisiting past examples
Scraping Amazon
```{r}
url = "https://www.amazon.com/product-reviews/B00005JNBQ/ref=cm_cr_arp_d_viewopt_rvwer?ie=UTF8&reviewerType=avp_only_reviews&sortBy=recent&pageNumber=1"

html = read_html(url)

review_profile = 
    html %>%
    html_nodes(".a-profile-content") %>%
    html_text()
  
  review_stars = 
    html %>%
    html_nodes(".review-rating") %>%
    html_text() %>%
    str_extract("^\\d") %>%
    as.numeric()
  
  review_date = 
    html %>%
    html_nodes(".review-date") %>%
    html_text() 

reviews=
    tibble(
    profile = review_profile,
    stars = review_stars,
    date = review_date
)
reviews
```
Let's turn that code into a function 
```{r}
read_page_reviews = function(url) {
# input is the url 
  html = read_html(url)
  
  review_profile = 
    html %>%
    html_nodes(".a-profile-content") %>%
    html_text()
  
  review_stars = 
    html %>%
    html_nodes(".review-rating") %>%
    html_text() %>%
    str_extract("^\\d") %>%
    as.numeric()
  
  review_date = 
    html %>%
    html_nodes(".review-date") %>%
    html_text() 
  
  reviews=
    tibble(
    profile = review_profile,
    stars = review_stars,
    date = review_date
  )
  # body and operations   
  reviews
# return objects 
}

dynamite_url = "https://www.amazon.com/product-reviews/B00005JNBQ/ref=cm_cr_arp_d_viewopt_rvwer?ie=UTF8&reviewerType=avp_only_reviews&sortBy=recent&pageNumber=1"
read_page_reviews(dynamite_url)
```
read in reviews from a few pages and combine the results
```{r}
url_base = "https://www.amazon.com/product-reviews/B00005JNBQ/ref=cm_cr_arp_d_viewopt_rvwer?ie=UTF8&reviewerType=avp_only_reviews&sortBy=recent&pageNumber="
vec_urls = str_c(url_base, 1:5)

dynamite_reviews = bind_rows(
  read_page_reviews(vec_urls[1]),
  read_page_reviews(vec_urls[2]),
  read_page_reviews(vec_urls[3]),
  read_page_reviews(vec_urls[4]),
  read_page_reviews(vec_urls[5])
)
```
## Loading LoTR data
```{r}
lotr_load_and_tidy = function(path, cell_range, movie_name) {
  
  df = readxl::read_excel(path, range = cell_range) |>
    janitor::clean_names() |>
    gather(key = sex, value = words, female:male) |>
    mutate(race = str_to_lower(race),
           movie = movie_name)
  
  df
  
}
```

## NSDUH
```{r}
nsduh_url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"

nsduh_html = read_html(nsduh_url)

data_marj = 
  nsduh_html |> 
  html_table() |> 
  nth(1) |>
  slice(-1) |> 
  select(-contains("P Value")) |>
  pivot_longer(
    -State,
    names_to = "age_year", 
    values_to = "percent") |>
  separate(age_year, into = c("age", "year"), sep = "\\(") |>
  mutate(
    year = str_replace(year, "\\)", ""),
    percent = str_replace(percent, "[a-c]$", ""),
    percent = as.numeric(percent)) |>
  filter(!(State %in% c("Total U.S.", "Northeast", "Midwest", "South", "West")))
```

```{r}
nsduh_table <- function(html, table_num, table_name) {
  
  table = 
    html |> 
    html_table() |> 
    nth(table_num) |>
    slice(-1) |> 
    select(-contains("P Value")) |>
    pivot_longer(
      -State,
      names_to = "age_year", 
      values_to = "percent") |>
    separate(age_year, into = c("age", "year"), sep = "\\(") |>
    mutate(
      year = str_replace(year, "\\)", ""),
      percent = str_replace(percent, "[a-c]$", ""),
      percent = as.numeric(percent),
      name = table_name) |>
    filter(!(State %in% c("Total U.S.", "Northeast", "Midwest", "South", "West")))
}
```
get a few different tables and combine the results
```{r}
nsduh_results = 
  bind_rows(
    nsduh_table(nsduh_html, 1, "marj_one_year"),
    nsduh_table(nsduh_html, 4, "cocaine_one_year"),
    nsduh_table(nsduh_html, 5, "heroin_one_year")
  )
```

```{r}
nsduh_results |> 
  filter(age == "26+") |> 
  mutate(name = fct_inorder(name)) |> 
  group_by(name, year) |> 
  mutate(State = fct_reorder(State, percent)) |> 
  ungroup() |> 
  ggplot(aes(x = State, y = percent, color = year)) + 
    geom_point() + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  facet_grid(. ~ name)
```

## Mean scoping example 
```{r}
f = function(x) {
  z = x + y
  z
}

x = 1
y = 2

f(x = y)
# y is the needed value for x 
```

## Functions as arguments
```{r}
x_vec = rnorm(25, 0, 1)

my_summary = function(x, summ_func) {
  summ_func(x)
}
mean(x_vec)
median(x_vec)
my_summary(x_vec, mean)
# reorder summ_func according to the mean 
```
